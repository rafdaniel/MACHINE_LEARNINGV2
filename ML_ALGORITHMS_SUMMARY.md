# Machine Learning Algorithms Summary

## Overview

This character guessing game project features **6 different machine learning algorithms**, each serving a specific purpose in character identification, difficulty prediction, and classification tasks. The system provides a comprehensive ML pipeline from simple nearest-neighbor approaches to advanced deep learning.

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHARACTER GUESSING GAME                       â”‚
â”‚                     (Express + React Frontend)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Express Server (Port 8080)                          â”‚
â”‚              API Routes: /api/ml/*                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Flask ML Service (Port 5000)                        â”‚
â”‚         6 Machine Learning Models + Training Pipeline            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”œâ”€â”€ K-NN (K-Nearest Neighbors)
â”œâ”€â”€ Linear Regression
â”œâ”€â”€ Naive Bayes
â”œâ”€â”€ SVM (Support Vector Machine)
â”œâ”€â”€ Decision Tree
â””â”€â”€ ANN (Artificial Neural Network)
```

---

## Algorithms Breakdown

### 1. K-Nearest Neighbors (K-NN)
**Color**: ğŸŸ¢ Green  
**Added**: Initial Implementation  
**Complexity**: Simple

#### Purpose
Character identification by finding similar characters based on semantic embeddings.

#### How It Works
- Uses sentence embeddings to convert character descriptions into vectors
- Finds the K nearest neighbors in the embedding space
- Returns top matching characters with similarity scores

#### Use Cases
- Quick character lookups
- Finding similar characters
- Baseline predictions

#### Technical Details
- **Model**: Sentence Transformers (all-MiniLM-L6-v2)
- **Distance Metric**: Cosine similarity
- **K Value**: Configurable (default: 5)
- **Training Time**: <5 seconds (embedding generation)
- **Prediction Time**: ~50ms

#### Strengths
âœ… Fast training and prediction  
âœ… Works well with small datasets  
âœ… Interpretable results  
âœ… No assumptions about data distribution

#### Weaknesses
âŒ Requires storing all training data  
âŒ Sensitive to irrelevant features  
âŒ No probability estimates

---

### 2. Linear Regression
**Color**: ğŸŸ¡ Yellow  
**Added**: Initial Implementation  
**Complexity**: Simple

#### Purpose
Predicts character difficulty score based on quantifiable attributes.

#### How It Works
- Extracts numerical features from character data
- Learns linear relationships between features and difficulty
- Outputs a continuous difficulty score (0-10)

#### Use Cases
- Game difficulty balancing
- Character complexity scoring
- Feature importance analysis

#### Technical Details
- **Model**: scikit-learn LinearRegression
- **Features**: powers_count, name_length, quote_length, description_length, source_length
- **Output Range**: 0-10 (clipped)
- **Training Time**: <1 second
- **Prediction Time**: <1ms

#### Strengths
âœ… Very fast training  
âœ… Interpretable coefficients  
âœ… Direct feature-to-difficulty mapping  
âœ… No hyperparameters to tune

#### Weaknesses
âŒ Assumes linear relationships  
âŒ Cannot capture complex interactions  
âŒ May overfit with many features

---

### 3. Naive Bayes
**Color**: ğŸŸ£ Magenta  
**Added**: Initial Implementation  
**Complexity**: Simple to Moderate

#### Purpose
Classifies characters by genre and universe using probabilistic text analysis.

#### How It Works
- Uses TF-IDF to convert text into numerical features
- Applies Bayes' theorem with independence assumption
- Trains separate classifiers for genre and universe

#### Use Cases
- Genre classification
- Universe identification
- Quick text-based categorization

#### Technical Details
- **Model**: MultinomialNB (scikit-learn)
- **Vectorizer**: TfidfVectorizer (5000 features)
- **Outputs**: Genre + Universe predictions
- **Training Time**: ~2-3 seconds
- **Prediction Time**: ~10ms

#### Performance
- **Genre Accuracy**: ~100% (often perfect)
- **Universe Accuracy**: ~63-72%

#### Strengths
âœ… Fast and efficient  
âœ… Works well with text data  
âœ… Provides probability estimates  
âœ… Handles high-dimensional data

#### Weaknesses
âŒ Independence assumption (naive)  
âŒ Cannot capture word relationships  
âŒ Requires sufficient training data per class

---

### 4. Support Vector Machine (SVM)
**Color**: ğŸ”µ Blue  
**Added**: Phase 2 (Recent Addition)  
**Complexity**: Moderate to Advanced

#### Purpose
Advanced character classification with margin-based decision boundaries and calibrated confidence scores.

#### How It Works
- Uses TF-IDF for feature extraction
- Finds optimal hyperplane to separate classes
- Applies kernel trick for non-linear boundaries
- Calibrates probabilities using Platt scaling

#### Use Cases
- High-accuracy character predictions
- Confidence-based decision making
- Handling overlapping character descriptions

#### Technical Details
- **Model**: SVC (scikit-learn) + CalibratedClassifierCV
- **Kernel**: RBF (Radial Basis Function) or Linear
- **Vectorizer**: TfidfVectorizer (5000 features)
- **C Parameter**: 1.0 (regularization)
- **Gamma**: 'scale' (RBF bandwidth)
- **Training Time**: ~5-10 seconds
- **Prediction Time**: ~20-30ms

#### Strengths
âœ… Excellent for high-dimensional data  
âœ… Memory efficient (support vectors only)  
âœ… Effective with clear margin of separation  
âœ… Calibrated probability estimates  
âœ… Multiple kernel options

#### Weaknesses
âŒ Longer training time  
âŒ Requires careful hyperparameter tuning  
âŒ Less interpretable than Decision Trees  
âŒ Struggles with very large datasets

---

### 5. Decision Tree
**Color**: ğŸŸ  Dark Yellow  
**Added**: Phase 3 (Recent Addition)  
**Complexity**: Moderate

#### Purpose
Interpretable classification and regression with visual decision rules and feature importance.

#### How It Works
- Builds tree structure of if-then-else rules
- Splits data based on feature thresholds
- Dual system: Classifier for characters, Regressor for difficulty
- Generates visualization of decision paths

#### Use Cases
- Explainable predictions
- Feature importance analysis
- Visual debugging of model logic
- Difficulty prediction with tree-based rules

#### Technical Details
- **Models**: DecisionTreeClassifier + DecisionTreeRegressor
- **Max Depth**: 10 (prevents overfitting)
- **Min Samples Split**: 5
- **Min Samples Leaf**: 2
- **Features**: Engineered (powers_count, name_length, universe, genre, etc.)
- **Training Time**: ~3-5 seconds
- **Prediction Time**: ~5ms
- **Visualization**: matplotlib tree plots

#### Strengths
âœ… Highly interpretable (visual tree)  
âœ… No feature scaling required  
âœ… Handles both numerical and categorical data  
âœ… Feature importance scores  
âœ… Can model non-linear relationships  
âœ… Fast predictions

#### Weaknesses
âŒ Prone to overfitting without pruning  
âŒ Unstable (small data changes = different tree)  
âŒ Can create biased trees with imbalanced data  
âŒ Not as accurate as ensemble methods

---

### 6. Artificial Neural Network (ANN)
**Color**: ğŸ”´ Red  
**Added**: Phase 4 (Final Addition)  
**Complexity**: Advanced (Deep Learning)

#### Purpose
State-of-the-art character classification and difficulty prediction using deep learning with semantic embeddings.

#### How It Works
- Generates 384-dimensional sentence embeddings using transformers
- Combines embeddings with engineered features (391 total)
- Processes through 3 hidden layers (256 â†’ 128 â†’ 64 neurons)
- Dual output: Character classification + Difficulty regression
- Uses Adam optimizer with early stopping

#### Use Cases
- Maximum accuracy predictions
- Complex pattern recognition
- Semantic understanding of character descriptions
- Production-ready AI predictions

#### Technical Details
- **Models**: MLPClassifier + MLPRegressor (scikit-learn)
- **Architecture**: 391 â†’ 256 â†’ 128 â†’ 64 â†’ Output
- **Embedding Model**: SentenceTransformer 'all-MiniLM-L6-v2'
- **Activation**: ReLU
- **Optimizer**: Adam (learning_rate=0.001)
- **Regularization**: L2 (alpha=0.0001) + Early stopping
- **Max Iterations**: 300 epochs
- **Features**: 384 embeddings + 5 numerical + 2 categorical
- **Training Time**: 1-3 minutes (includes embedding generation)
- **Prediction Time**: ~50-100ms

#### Performance
- **Character Classification**: 70-85% test accuracy
- **Difficulty Regression**: RÂ² of 0.60-0.80

#### Strengths
âœ… Highest accuracy potential  
âœ… Semantic text understanding  
âœ… Learns complex non-linear patterns  
âœ… Dual prediction capability  
âœ… Probability distributions  
âœ… Scalable with more data  
âœ… Modern deep learning techniques

#### Weaknesses
âŒ Longest training time (1-3 minutes)  
âŒ Less interpretable (black box)  
âŒ Requires more computational resources  
âŒ Needs larger datasets for best performance  
âŒ More memory usage

---

## Algorithm Comparison Table

| Algorithm | Training Time | Prediction Time | Accuracy | Interpretability | Best For |
|-----------|--------------|----------------|----------|------------------|----------|
| **K-NN** | <5s | ~50ms | Moderate | High | Quick lookups, small datasets |
| **Linear Regression** | <1s | <1ms | Low-Moderate | Very High | Difficulty scoring, feature analysis |
| **Naive Bayes** | ~3s | ~10ms | Moderate-High | Moderate | Text classification, fast predictions |
| **SVM** | ~10s | ~30ms | High | Low | High-dimensional data, margin-based |
| **Decision Tree** | ~5s | ~5ms | Moderate-High | Very High | Explainable AI, rule extraction |
| **ANN** | 1-3min | ~100ms | **Highest** | Low | Production AI, maximum accuracy |

---

## When to Use Each Algorithm

### ğŸ¯ For Character Identification

1. **K-NN**: Quick baseline, finding similar characters
2. **Naive Bayes**: Fast genre/universe classification
3. **SVM**: High accuracy with confidence scores
4. **Decision Tree**: Explainable predictions with rules
5. **ANN**: Maximum accuracy with semantic understanding

### ğŸ“Š For Difficulty Prediction

1. **Linear Regression**: Simple feature-to-difficulty mapping
2. **Decision Tree Regressor**: Rule-based difficulty with visualization
3. **ANN Regressor**: Most accurate difficulty scores

### ğŸ” For Analysis & Debugging

1. **Decision Tree**: Visual tree, feature importance
2. **Linear Regression**: Feature coefficients
3. **Naive Bayes**: Probability distributions per class

### âš¡ For Speed Requirements

**Fastest Training**: Linear Regression > K-NN > Naive Bayes > Decision Tree > SVM > ANN  
**Fastest Prediction**: Linear Regression > Decision Tree > Naive Bayes > K-NN > SVM > ANN

---

## API Endpoints Summary

### Health & Status
- `GET /health` - Check all models status

### K-NN Endpoints
- `POST /train-knn` - Train K-NN model
- `POST /predict-knn` - Predict character with K-NN

### Linear Regression Endpoints
- `POST /train-lr` - Train Linear Regression
- `POST /predict-difficulty` - Predict difficulty score

### Naive Bayes Endpoints
- `POST /train-naive-bayes` - Train Naive Bayes
- `POST /predict-genre` - Predict genre
- `POST /predict-universe` - Predict universe

### SVM Endpoints
- `POST /train-svm` - Train SVM model
- `POST /predict-svm` - Predict character with SVM
- `GET /svm-info` - Get SVM model information

### Decision Tree Endpoints
- `POST /train-decision-tree` - Train Decision Tree (classifier + regressor)
- `POST /predict-decision-tree` - Predict character with Decision Tree
- `POST /predict-difficulty-tree` - Predict difficulty with Decision Tree
- `GET /decision-tree-info` - Get model information
- `GET /decision-tree-rules` - Get decision rules as text
- `POST /visualize-decision-tree` - Generate tree visualization
- `GET /feature-importance-tree` - Get feature importance scores

### ANN Endpoints
- `POST /train-ann` - Train ANN models (classifier + regressor)
- `POST /predict-ann` - Predict character with ANN
- `POST /predict-difficulty-ann` - Predict difficulty with ANN
- `GET /ann-info` - Get ANN model information

**Total Endpoints**: 30+ endpoints across 6 algorithms

---

## Training Pipeline

### Auto-Training on Startup

When the Flask server starts, all models are automatically trained in sequence:

```
1. Loading character dataset...
2. Training K-NN model... âœ“ (3s)
3. Training Linear Regression... âœ“ (1s)
4. Training Naive Bayes... âœ“ (3s)
5. Training SVM... âœ“ (10s)
6. Training Decision Tree... âœ“ (5s)
7. Training ANN... âœ“ (90s)
   - Generating embeddings...
   - Training classifier...
   - Training regressor...

Total training time: ~2 minutes
All models saved to .pkl files
```

### Model Persistence

All trained models are saved as pickle files:
- `knn_model.pkl`
- `lr_model.pkl`
- `naive_bayes_model.pkl`
- `svm_model.pkl`
- `decision_tree_model.pkl`
- `ann_model.pkl`

Subsequent server restarts load pre-trained models (<1 second).

---

## Technology Stack

### Python Libraries
- **scikit-learn**: Core ML algorithms (K-NN, LR, NB, SVM, DT, ANN/MLP)
- **sentence-transformers**: Semantic embeddings (K-NN, ANN)
- **Flask**: ML service API server
- **NumPy**: Numerical computations
- **matplotlib**: Decision Tree visualization
- **colorama**: Colored terminal output for tests

### Backend Integration
- **Express (Node.js)**: Main API server (port 8080)
- **Flask (Python)**: ML service (port 5000)
- **React**: Frontend UI

---

## Performance Metrics

### Model Accuracy (on test set)

| Model | Metric | Score |
|-------|--------|-------|
| K-NN | Top-5 Accuracy | Varies (similarity-based) |
| Linear Regression | RÂ² Score | 0.40-0.70 |
| Naive Bayes (Genre) | Accuracy | ~100% |
| Naive Bayes (Universe) | Accuracy | ~63-72% |
| SVM | Accuracy | 70-80% |
| Decision Tree (Classifier) | Accuracy | 60-75% |
| Decision Tree (Regressor) | RÂ² Score | 0.50-0.70 |
| ANN (Classifier) | Accuracy | **70-85%** |
| ANN (Regressor) | RÂ² Score | **0.60-0.80** |

*Note: Actual performance depends on dataset size and quality*

---

## Testing

Each algorithm has a dedicated test script:
- `test_knn.py` - K-NN tests
- `test_lr_quick.py` - Linear Regression tests
- `test_naive_bayes.py` - Naive Bayes tests (5/5 tests passed âœ“)
- `test_svm.py` - SVM tests
- `test_decision_tree.py` - Decision Tree tests (7/7 tests passed âœ“)
- `test_ann.py` - ANN tests (5 comprehensive tests)

---

## Documentation

Detailed algorithm-specific documentation:
- `SVM_SUMMARY.md` - Support Vector Machine guide
- `DECISION_TREE_SUMMARY.md` - Decision Tree guide
- `ANN_SUMMARY.md` - Artificial Neural Network guide
- `LINEAR_REGRESSION_SUMMARY.md` - Linear Regression guide
- `NAIVE_BAYES_SUMMARY.md` - Naive Bayes guide

---

## Progression of ML Sophistication

```
Simple â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Advanced

K-NN          Linear Regression      Naive Bayes
(Distance)    (Linear)               (Probabilistic)
   â”‚              â”‚                        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
           SVM            Decision Tree
       (Margin-based)    (Rule-based)
                  â”‚
                 ANN
          (Deep Learning)
```

---

## Future Enhancements

### Potential Improvements
1. **Ensemble Methods**: Combine predictions from multiple models
2. **Hyperparameter Tuning**: Grid search for optimal parameters
3. **Feature Engineering**: Add more sophisticated features
4. **Online Learning**: Update models as players interact
5. **A/B Testing**: Compare model performance in production
6. **Model Explainability**: SHAP values, LIME interpretations
7. **Transfer Learning**: Use larger pre-trained models (BERT, GPT)

---

## Conclusion

This project demonstrates a **comprehensive machine learning pipeline** with 6 distinct algorithms, each serving specific purposes:

- **K-NN** provides quick similarity-based lookups
- **Linear Regression** offers fast difficulty scoring
- **Naive Bayes** enables efficient text classification
- **SVM** delivers high-accuracy margin-based predictions
- **Decision Tree** ensures interpretable rule-based decisions
- **ANN** achieves state-of-the-art accuracy with deep learning

Together, these algorithms create a **robust, flexible, and production-ready ML system** for character guessing and analysis. The diversity of approaches ensures optimal performance across different use cases, from fast predictions to maximum accuracy.

**Total System Capabilities**:
- âœ… 6 Machine Learning Algorithms
- âœ… 30+ API Endpoints
- âœ… Dual Backend Architecture (Express + Flask)
- âœ… Auto-training on Startup
- âœ… Model Persistence (Pickle)
- âœ… Comprehensive Testing
- âœ… Detailed Documentation
- âœ… Production-Ready Integration

ğŸ‰ **A complete, enterprise-grade ML system for character analysis and prediction!**
